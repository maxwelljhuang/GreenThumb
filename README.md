# GreenThumb Discovery MVP

A production-ready data ingestion and discovery platform designed to handle CSV data ingestion, validation, deduplication, and storage for 300k+ products, with future capabilities for embeddings and semantic search.

## ğŸŒŸ Features

- **Data Ingestion Pipeline**: Robust CSV parsing with validation and deduplication
- **REST API**: FastAPI-based API for product data access
- **Database Management**: PostgreSQL with Alembic migrations
- **Data Transformations**: DBT for data modeling and transformations
- **Async Task Processing**: Celery for background jobs
- **Production Ready**: Docker containerization, comprehensive testing, CI/CD
- **Future Ready**: Structured for vector embeddings and semantic search

## ğŸ“‹ Requirements

- Python 3.11+
- PostgreSQL 15+
- Redis 7+
- Docker & Docker Compose (recommended)
- Node.js 18+ (for frontend, optional)

## ğŸš€ Quick Start

### Using Docker (Recommended)

```bash
# Clone the repository
git clone <repository-url>
cd discovery-mvp

# Copy environment variables
cp .env.example .env
# Edit .env with your configuration

# Start all services
make docker-up

# Run database migrations
make migrate

# Access the API
open http://localhost:8000/docs
```

### Local Development

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
make install-dev

# Set up environment
cp .env.example .env
# Edit .env with your configuration

# Run database migrations
make migrate

# Start the development server
make run-dev

# In another terminal, start Celery worker
make celery-worker
```

## ğŸ“ Project Structure

```
discovery-mvp/
â”œâ”€â”€ backend/                 # Python backend package
â”‚   â”œâ”€â”€ ingestion/          # Data ingestion logic
â”‚   â”‚   â”œâ”€â”€ parsers/        # CSV and data parsers
â”‚   â”‚   â”œâ”€â”€ validators/     # Data validation
â”‚   â”‚   â””â”€â”€ deduplicators/  # Deduplication logic
â”‚   â”œâ”€â”€ models/             # Database models
â”‚   â”œâ”€â”€ database/           # Database connection & operations
â”‚   â”œâ”€â”€ api/                # REST API endpoints
â”‚   â”‚   â”œâ”€â”€ routes/         # API route handlers
â”‚   â”‚   â””â”€â”€ schemas/        # Pydantic schemas
â”‚   â”œâ”€â”€ search/             # Search functionality (future)
â”‚   â”‚   â”œâ”€â”€ embeddings/     # Vector embeddings
â”‚   â”‚   â””â”€â”€ indices/        # Search indices
â”‚   â””â”€â”€ utils/              # Utility functions
â”œâ”€â”€ data/                   # Data directories
â”‚   â”œâ”€â”€ raw/                # Raw CSV files
â”‚   â”œâ”€â”€ processed/          # Processed data
â”‚   â”œâ”€â”€ temp/               # Temporary files
â”‚   â”œâ”€â”€ test/               # Test data
â”‚   â”œâ”€â”€ embeddings/         # Vector embeddings (future)
â”‚   â””â”€â”€ indices/            # Search indices (future)
â”œâ”€â”€ database/               # Database management
â”‚   â”œâ”€â”€ migrations/         # Alembic migrations
â”‚   â”œâ”€â”€ schemas/            # Database schemas
â”‚   â”œâ”€â”€ seeds/              # Seed data
â”‚   â””â”€â”€ backups/            # Database backups
â”œâ”€â”€ dbt-project/            # DBT transformations
â”‚   â”œâ”€â”€ models/             # DBT models
â”‚   â”‚   â”œâ”€â”€ staging/        # Staging models
â”‚   â”‚   â””â”€â”€ marts/          # Business logic models
â”‚   â”œâ”€â”€ macros/             # DBT macros
â”‚   â”œâ”€â”€ tests/              # DBT tests
â”‚   â””â”€â”€ seeds/              # DBT seed data
â”œâ”€â”€ tests/                  # Test suite
â”‚   â”œâ”€â”€ unit/               # Unit tests
â”‚   â”œâ”€â”€ integration/        # Integration tests
â”‚   â””â”€â”€ fixtures/           # Test fixtures
â”œâ”€â”€ docs/                   # Documentation
â”‚   â”œâ”€â”€ api/                # API documentation
â”‚   â”œâ”€â”€ architecture/       # Architecture docs
â”‚   â””â”€â”€ deployment/         # Deployment guides
â”œâ”€â”€ scripts/                # Utility scripts
â”‚   â”œâ”€â”€ ingestion/          # Ingestion scripts
â”‚   â”œâ”€â”€ maintenance/        # Maintenance scripts
â”‚   â”œâ”€â”€ deployment/         # Deployment scripts
â”‚   â””â”€â”€ monitoring/         # Monitoring scripts
â”œâ”€â”€ frontend/               # Frontend application (future)
â”‚   â””â”€â”€ src/                # React/Next.js source
â”œâ”€â”€ logs/                   # Application logs
â”œâ”€â”€ .github/                # GitHub Actions workflows
â”œâ”€â”€ docker-compose.yml      # Docker services configuration
â”œâ”€â”€ Dockerfile              # Docker image definition
â”œâ”€â”€ Makefile               # Development commands
â””â”€â”€ pyproject.toml         # Python project configuration
```

## ğŸ”§ Development

### Common Commands

```bash
# Install dependencies
make install-dev

# Run tests
make test

# Run linting
make lint

# Format code
make format

# Type check
make type-check

# Run all quality checks
make quality

# Start development server
make run-dev

# Database migrations
make migrate
make migrate-create MSG="description"

# Docker commands
make docker-up
make docker-down
make docker-logs

# DBT commands
make dbt-run
make dbt-test
make dbt-docs
```

### Running Tests

```bash
# Run all tests with coverage
make test

# Run specific test file
pytest tests/unit/ingestion/test_parsers.py -v

# Run tests with specific markers
pytest -m unit          # Unit tests only
pytest -m integration   # Integration tests only
pytest -m "not slow"    # Skip slow tests

# Run tests in watch mode
make test-watch
```

### Code Quality

This project uses:
- **Black** for code formatting
- **isort** for import sorting
- **flake8** for linting
- **mypy** for type checking
- **pytest** for testing
- **pre-commit** hooks for quality enforcement

```bash
# Set up pre-commit hooks
pre-commit install

# Run pre-commit on all files
make pre-commit
```

## ğŸ“Š Database

### Migrations

```bash
# Create a new migration
make migrate-create MSG="add products table"

# Apply migrations
make migrate

# Rollback one migration
make migrate-downgrade

# View migration history
make migrate-history
```

### DBT Transformations

```bash
# Run DBT models
make dbt-run

# Run DBT tests
make dbt-test

# Generate and view DBT docs
make dbt-docs
```

## ğŸ³ Docker

### Services

- **postgres**: PostgreSQL database (port 5432)
- **redis**: Redis cache (port 6379)
- **api**: FastAPI application (port 8000)
- **celery-worker**: Celery worker for async tasks
- **celery-beat**: Celery beat for scheduled tasks

### Commands

```bash
# Start all services
docker-compose up -d

# View logs
docker-compose logs -f api

# Access API container shell
docker-compose exec api /bin/bash

# Access database
docker-compose exec postgres psql -U postgres -d greenthumb_dev

# Stop all services
docker-compose down

# Clean up everything
docker-compose down -v --rmi all
```

## ğŸ“ˆ API Documentation

Once the server is running, visit:
- Swagger UI: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc

## ğŸ§ª Testing

The project follows pytest conventions with comprehensive test coverage:

```bash
# Run all tests
pytest

# Run with coverage report
pytest --cov=backend --cov-report=html

# Run specific test types
pytest -m unit
pytest -m integration
pytest -m "not slow"

# Run in parallel
pytest -n auto
```

## ğŸ“ Environment Variables

Key environment variables (see `.env.example` for full list):

```bash
# Application
APP_ENV=development
DEBUG=true

# Database
DATABASE_URL=postgresql://user:pass@localhost:5432/dbname

# API
API_HOST=0.0.0.0
API_PORT=8000

# Redis
REDIS_URL=redis://localhost:6379/0
```

## ğŸš¢ Deployment

See [docs/deployment/README.md](docs/deployment/README.md) for detailed deployment instructions.

### Production Checklist

- [ ] Set production environment variables
- [ ] Configure database connection
- [ ] Set up database backups
- [ ] Configure logging and monitoring
- [ ] Set up SSL/TLS certificates
- [ ] Configure rate limiting
- [ ] Set up error tracking (Sentry)
- [ ] Review security settings

## ğŸ“š Documentation

- [API Documentation](docs/api/README.md)
- [Architecture Overview](docs/architecture/README.md)
- [Deployment Guide](docs/deployment/README.md)

## ğŸ¤ Contributing

1. Create a feature branch
2. Make your changes
3. Run quality checks: `make quality`
4. Submit a pull request

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ”® Future Enhancements

- [ ] Vector embeddings generation
- [ ] Semantic search capabilities
- [ ] Frontend application
- [ ] API authentication & authorization
- [ ] Advanced analytics dashboard
- [ ] Real-time data ingestion
- [ ] Multi-source data ingestion

## ğŸ“ Support

For questions and support, please open an issue in the repository.

---

Built with â¤ï¸ by the GreenThumb Team

